---
title: "p8105_hw3_rj2543"
author: "rj2543"
date: "October 10, 2018"
output: github_document
---

```{r set up, include = FALSE}
library(tidyverse)
library(ggridges)
library(p8105.datasets)
library(patchwork)
```

# Problem 1

This problem uses the BRFSS data. DO NOT include this dataset in your local data directory; instead, load the data from the  p8105.datasets package.

First, do some data cleaning:

* format the data to use appropriate variable names;

* focus on the “Overall Health” topic

* include only responses from “Excellent” to “Poor”

* organize responses as a factor taking levels ordered from “Excellent” to “Poor”

```{r problem 1 data cleaning}
data("brfss_smart2010")

brfss = brfss_smart2010 %>% 
  janitor::clean_names() %>% # clean up variable names
  filter(topic == "Overall Health") %>% # focus on "Overall Health" topic
  filter(response %in% c("Excellent", "Very good", "Good", "Fair", "Poor")) %>%  # include responses from "Excellent" to "Poor"
  mutate(response = forcats::fct_relevel(response, c("Excellent", "Very Good", "Good", "Fair", "Poor"))) # "response" as factor taking levels ordered, can we just use "mutate(response = factor(response, levels = c("Excellent", "Very Good", "Good", "Fair", "Poor")))"?
  
brfss
```

Using this dataset, do or answer the following (commenting on the results of each):

* In 2002, which states were observed at 7 locations?

```{r state observation}
brfss %>% 
  filter(year == 2002) %>% # fix "year" to 2002
  distinct(locationabbr, locationdesc) %>% # "distinct" locations
  count(locationabbr) %>% # count observations
  filter(n == 7)
```

```{r state observation another approach}
brfss %>% 
  filter(year == 2002) %>% 
  group_by(locationabbr) %>% 
  summarise(n = n_distinct(locationdesc)) %>% 
  filter(n == 7)
```

**In 2002, three states were observed at 7 locations, which are CT, FL and NC in specific.**

* Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.

```{r spaghetti plot}
brfss %>% 
  group_by(year, locationabbr) %>% 
  summarise(n = n_distinct(locationdesc))%>% 
  ggplot(aes(x = locationabbr, y = n, color = year)) + 
  geom_line()
  
```


* Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

```{r table}
brfss %>% 
  filter(year %in% c(2002, 2006, 2010), locationabbr == "NY", response == "Excellent") %>% 
  group_by(year) %>% 
  summarise(mean = mean(data_value),
            sd = sd(data_value))
```


* For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time

```{r five-panel plot}
brfss %>% 
  group_by(year, locationabbr, response) %>% 
  summarise(mean = mean(data_value)) %>% 
  ggplot(aes(x = locationabbr, y = mean, color = year)) + 
  geom_line() + 
  facet_grid(~response)
```

# Problem 2

This problem uses the Instacart data. DO NOT include this dataset in your local data directory; instead, load the data from the  p8105.datasets package (it’s called instacart).

```{r problem 2 data import}
data("instacart")

instacart = instacart %>% 
  janitor::clean_names()

instacart
```

The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and giving illstrative examples of observations. 

**The dataset is derived from the original "instacart" data. The dataset has `r nrow(instacart)` rows and `r ncol(instacart)` columns. The structure of the data is `r class(instacart)`. Variables include `r names(instacart)`. Key variables descriptions can be find on** http://p8105.com/dataset_instacart.html:

* order_id: order identifier
* product_id: product identifier
* add_to_cart_order: order in which each product was added to cart
* reordered: 1 if this prodcut has been ordered by this user in the past, 0 otherwise
* user_id: customer identifier
* eval_set: which evaluation set this order belongs in (Note that the data for use in this class is exclusively from the “train”  eval_set)
* order_number: the order sequence number for this user (1=first, n=nth)
* order_dow: the day of the week on which the order was placed
* order_hour_of_day: the hour of the day on which the order was placed
* days_since_prior_order: days since the last order, capped at 30, NA if order_number=1
* product_name: name of the product
* aisle_id: aisle identifier
* department_id: department identifier
* aisle: the name of the aisle
* department: the name of the department

**Some illustrative examples are shown below:**
```{r illustrative examples, echo = FALSE}
head(instacart)

tail(instacart)
```

Then, do or answer the following (commenting on the results of each):

* How many aisles are there, and which aisles are the most items ordered from?

```{r aisle count}
instacart %>% 
  distinct(aisle_id) %>% # distinct aisles
  count()

instacart %>%
  group_by(aisle_id, aisle) %>% 
  summarise(n_obs = n()) %>% # count observations of each aisle
  arrange(desc(n_obs)) # arrange number of observations in a descending order

```
** There are 134 dinstinct aisles. The top 3 popular aisles are 83-fresh vegetables (150609 items), 24-fresh fruits (150473 items) and 123-packaged vegetables fruits (78493 items). **

* Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.

```{r aisle plot}
instacart %>%
  group_by(aisle_id, aisle) %>% 
  summarise(n_obs = n()) %>% # count observations of each aisle
  # mutate(aisle = forcats::fct_reorder(aisle, n_obs)) %>% 
  ggplot() + 
  geom_point(aes(x = aisle, y = n_obs)) + 
  labs(
    title = "Popularity of Aisles",
    x = "aisle",
    y = "number of items ordered",
    caption = "Data from instacart"
  )
  
```

* Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

```{r popular items}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>% 
  summarise(popularity = n()) %>% 
  group_by(aisle) %>% 
  filter(popularity == max(popularity)) %>% 
  knitr::kable()
```

* Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r mean hour}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarise(mean_hour = mean(order_hour_of_day)) %>% 
  spread(key = order_dow, value = mean_hour) %>% # produce a 2*7 table
  knitr::kable(digits = 1) # Do we need to round the mean hour in order to be more reader-friendly?
```


# Problem 3

This problem uses the NY NOAA data. DO NOT include this dataset in your local data directory; instead, load the data from the  p8105.datasets package (it’s called ny_noaa).

```{r problem 3 data import}
data("ny_noaa")

ny_noaa = ny_noaa %>% 
  janitor::clean_names() %>% 
  mutate(tmax = as.numeric(tmax), tmin = as.numeric(tmin)) # convert "tmax" & "tmin" from chr to numeric

ny_noaa
skimr::skim(ny_noaa)
```


The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and indicating the extent to which missing data is an issue. Then, do or answer the following (commenting on the results of each):

**The dataset is derived from the original data "ny_noaa". The dataset has `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. The structure of the data is `r class(ny_noaa)`. Key variables include `r names(ny_noaa)`. Descriptions about variables can be found at**
http://p8105.com/dataset_noaa.html: 

* id: Weather station ID
* date: Date of observation
* prcp: Precipitation (tenths of mm)
* snow: Snowfall (mm)
* snwd: Snow depth (mm)
* tmax: Maximum temperature (tenths of degrees C)
* tmin: Minimum temperature (tenths of degrees C)

**By using "skimr::skim(ny_noaa)", we can see that there are 145838 (`r 145838/2595176*100`%) missing data for prcp, 381221 (`r 381221/2595176*100`%) missing data for snow, 591786(`r 591786/2595176*100`%) missing data for snwd , 1134358 (`r 1134358/2595176*100`%) missing data for tmax and 1134420 (`r 1134420/2595176*100`%) missing data for tmin.**

* Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why?

```{r problem 3 data cleaning}
ny_noaa1 = ny_noaa %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>% # create separate variables for original "date"
  mutate(year = as.integer(year), month = as.integer(month), day = as.integer(day), prcp = prcp/10, tmax = tmax/10, tmin = tmin/10) # convert "year", "month", "day" from chr to int, convert "prcp", "tmax", "tmin" from tenths to units

ny_noaa1
```

```{r snowfall observation}
ny_noaa1 %>% 
  filter(!is.na(snow)) %>% # exclude NA
  group_by(snow) %>% # focus on snowfall
  summarise(n_obs = n()) %>% # count observations for each snowfall
  arrange(desc(n_obs)) # interested in most commonly observed values
```

** Despite NA and 0 mm, 25 mm, 13 mm and 51 mm are the top 3 most commonly observed values. **

* Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

```{r average tmax in Jan & Jul}
ny_noaa1 %>% 
  filter(month %in% c(1,7)) %>% 
  group_by(year, month) %>% 
  summarise(mean_tmax = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = mean_tmax)) +
  geom_line() +
  facet_grid(.~month)
```


* Make a two-panel plot showing 

(i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); 

(ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r 2-panel plot}
tmax_tmin = ny_noaa1 %>% 
  filter(!is.na(tmax) & !is.na(tmin)) %>% 
  ggplot(aes(x = tmax, y = tmin, color = year)) +
  geom_smooth(se = FALSE)

snowfall_distribution = ny_noaa1 %>% 
  mutate(year = as.factor(year)) %>% 
  filter(snow > 0 & snow < 100) %>% 
  ggplot(aes(x = year, y = snow)) +
  geom_violin(aes(fill = year)) +
  theme(legend.position = "bottom")

tmax_tmin/snowfall_distribution
```

