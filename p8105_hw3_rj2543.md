p8105\_hw3\_rj2543
================
rj2543
October 10, 2018

Problem 1
=========

This problem uses the BRFSS data. DO NOT include this dataset in your local data directory; instead, load the data from the p8105.datasets package.

First, do some data cleaning:

-   format the data to use appropriate variable names;

-   focus on the “Overall Health” topic

-   include only responses from “Excellent” to “Poor”

-   organize responses as a factor taking levels ordered from “Excellent” to “Poor”

``` r
data("brfss_smart2010")

brfss = brfss_smart2010 %>% 
  janitor::clean_names() %>% # clean up variable names
  filter(topic == "Overall Health") %>% # focus on "Overall Health" topic
  filter(response %in% c("Excellent", "Very good", "Good", "Fair", "Poor")) %>%  # include responses from "Excellent" to "Poor"
  mutate(response = forcats::fct_relevel(response, c("Excellent", "Very Good", "Good", "Fair", "Poor"))) # "response" as factor taking levels ordered, can we just use "mutate(response = factor(response, levels = c("Excellent", "Very Good", "Good", "Fair", "Poor")))"?
```

    ## Warning: Unknown levels in `f`: Very Good

``` r
brfss
```

    ## # A tibble: 10,625 x 23
    ##     year locationabbr locationdesc class topic question response
    ##    <int> <chr>        <chr>        <chr> <chr> <chr>    <fct>   
    ##  1  2010 AL           AL - Jeffer~ Heal~ Over~ How is ~ Excelle~
    ##  2  2010 AL           AL - Jeffer~ Heal~ Over~ How is ~ Very go~
    ##  3  2010 AL           AL - Jeffer~ Heal~ Over~ How is ~ Good    
    ##  4  2010 AL           AL - Jeffer~ Heal~ Over~ How is ~ Fair    
    ##  5  2010 AL           AL - Jeffer~ Heal~ Over~ How is ~ Poor    
    ##  6  2010 AL           AL - Mobile~ Heal~ Over~ How is ~ Excelle~
    ##  7  2010 AL           AL - Mobile~ Heal~ Over~ How is ~ Very go~
    ##  8  2010 AL           AL - Mobile~ Heal~ Over~ How is ~ Good    
    ##  9  2010 AL           AL - Mobile~ Heal~ Over~ How is ~ Fair    
    ## 10  2010 AL           AL - Mobile~ Heal~ Over~ How is ~ Poor    
    ## # ... with 10,615 more rows, and 16 more variables: sample_size <int>,
    ## #   data_value <dbl>, confidence_limit_low <dbl>,
    ## #   confidence_limit_high <dbl>, display_order <int>,
    ## #   data_value_unit <chr>, data_value_type <chr>,
    ## #   data_value_footnote_symbol <chr>, data_value_footnote <chr>,
    ## #   data_source <chr>, class_id <chr>, topic_id <chr>, location_id <chr>,
    ## #   question_id <chr>, respid <chr>, geo_location <chr>

Using this dataset, do or answer the following (commenting on the results of each):

-   In 2002, which states were observed at 7 locations?

``` r
brfss %>% 
  filter(year == 2002) %>% # fix "year" to 2002
  distinct(locationabbr, locationdesc) %>% # "distinct" locations
  count(locationabbr) %>% # count observations
  filter(n == 7)
```

    ## # A tibble: 3 x 2
    ##   locationabbr     n
    ##   <chr>        <int>
    ## 1 CT               7
    ## 2 FL               7
    ## 3 NC               7

``` r
brfss %>% 
  filter(year == 2002) %>% 
  group_by(locationabbr) %>% 
  summarise(n = n_distinct(locationdesc)) %>% 
  filter(n == 7)
```

    ## # A tibble: 3 x 2
    ##   locationabbr     n
    ##   <chr>        <int>
    ## 1 CT               7
    ## 2 FL               7
    ## 3 NC               7

**In 2002, three states were observed at 7 locations, which are CT, FL and NC in specific.**

-   Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.

``` r
brfss %>% 
  group_by(year, locationabbr) %>% 
  summarise(n = n_distinct(locationdesc))%>% 
  ggplot(aes(x = locationabbr, y = n, color = year)) + 
  geom_line()
```

![](p8105_hw3_rj2543_files/figure-markdown_github/spaghetti%20plot-1.png)

-   Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

``` r
brfss %>% 
  filter(year %in% c(2002, 2006, 2010), locationabbr == "NY", response == "Excellent") %>% 
  group_by(year) %>% 
  summarise(mean = mean(data_value),
            sd = sd(data_value))
```

    ## # A tibble: 3 x 3
    ##    year  mean    sd
    ##   <int> <dbl> <dbl>
    ## 1  2002  24.0  4.49
    ## 2  2006  22.5  4.00
    ## 3  2010  22.7  3.57

-   For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time

``` r
brfss %>% 
  group_by(year, locationabbr, response) %>% 
  summarise(mean = mean(data_value)) %>% 
  ggplot(aes(x = locationabbr, y = mean, color = year)) + 
  geom_line() + 
  facet_grid(~response)
```

    ## Warning: Removed 2 rows containing missing values (geom_path).

![](p8105_hw3_rj2543_files/figure-markdown_github/five-panel%20plot-1.png)

Problem 2
=========

This problem uses the Instacart data. DO NOT include this dataset in your local data directory; instead, load the data from the p8105.datasets package (it’s called instacart).

``` r
data("instacart")

instacart = instacart %>% 
  janitor::clean_names()

instacart
```

    ## # A tibble: 1,384,617 x 15
    ##    order_id product_id add_to_cart_ord~ reordered user_id eval_set
    ##       <int>      <int>            <int>     <int>   <int> <chr>   
    ##  1        1      49302                1         1  112108 train   
    ##  2        1      11109                2         1  112108 train   
    ##  3        1      10246                3         0  112108 train   
    ##  4        1      49683                4         0  112108 train   
    ##  5        1      43633                5         1  112108 train   
    ##  6        1      13176                6         0  112108 train   
    ##  7        1      47209                7         0  112108 train   
    ##  8        1      22035                8         1  112108 train   
    ##  9       36      39612                1         0   79431 train   
    ## 10       36      19660                2         1   79431 train   
    ## # ... with 1,384,607 more rows, and 9 more variables: order_number <int>,
    ## #   order_dow <int>, order_hour_of_day <int>,
    ## #   days_since_prior_order <int>, product_name <chr>, aisle_id <int>,
    ## #   department_id <int>, aisle <chr>, department <chr>

The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and giving illstrative examples of observations.

**The dataset is derived from the original "instacart" data. The dataset has 1384617 rows and 15 columns. The structure of the data is tbl\_df, tbl, data.frame. Variables include order\_id, product\_id, add\_to\_cart\_order, reordered, user\_id, eval\_set, order\_number, order\_dow, order\_hour\_of\_day, days\_since\_prior\_order, product\_name, aisle\_id, department\_id, aisle, department. Key variables descriptions can be find on** <http://p8105.com/dataset_instacart.html>:

-   order\_id: order identifier
-   product\_id: product identifier
-   add\_to\_cart\_order: order in which each product was added to cart
-   reordered: 1 if this prodcut has been ordered by this user in the past, 0 otherwise
-   user\_id: customer identifier
-   eval\_set: which evaluation set this order belongs in (Note that the data for use in this class is exclusively from the “train” eval\_set)
-   order\_number: the order sequence number for this user (1=first, n=nth)
-   order\_dow: the day of the week on which the order was placed
-   order\_hour\_of\_day: the hour of the day on which the order was placed
-   days\_since\_prior\_order: days since the last order, capped at 30, NA if order\_number=1
-   product\_name: name of the product
-   aisle\_id: aisle identifier
-   department\_id: department identifier
-   aisle: the name of the aisle
-   department: the name of the department

**Some illustrative examples are shown below:**

    ## # A tibble: 6 x 15
    ##   order_id product_id add_to_cart_ord~ reordered user_id eval_set
    ##      <int>      <int>            <int>     <int>   <int> <chr>   
    ## 1        1      49302                1         1  112108 train   
    ## 2        1      11109                2         1  112108 train   
    ## 3        1      10246                3         0  112108 train   
    ## 4        1      49683                4         0  112108 train   
    ## 5        1      43633                5         1  112108 train   
    ## 6        1      13176                6         0  112108 train   
    ## # ... with 9 more variables: order_number <int>, order_dow <int>,
    ## #   order_hour_of_day <int>, days_since_prior_order <int>,
    ## #   product_name <chr>, aisle_id <int>, department_id <int>, aisle <chr>,
    ## #   department <chr>

    ## # A tibble: 6 x 15
    ##   order_id product_id add_to_cart_ord~ reordered user_id eval_set
    ##      <int>      <int>            <int>     <int>   <int> <chr>   
    ## 1  3421063      13565                2         1  169679 train   
    ## 2  3421063      14233                3         1  169679 train   
    ## 3  3421063      35548                4         1  169679 train   
    ## 4  3421070      35951                1         1  139822 train   
    ## 5  3421070      16953                2         1  139822 train   
    ## 6  3421070       4724                3         1  139822 train   
    ## # ... with 9 more variables: order_number <int>, order_dow <int>,
    ## #   order_hour_of_day <int>, days_since_prior_order <int>,
    ## #   product_name <chr>, aisle_id <int>, department_id <int>, aisle <chr>,
    ## #   department <chr>

Then, do or answer the following (commenting on the results of each):

-   How many aisles are there, and which aisles are the most items ordered from?

``` r
instacart %>% 
  distinct(aisle_id) %>% # distinct aisles
  count()
```

    ## # A tibble: 1 x 1
    ##       n
    ##   <int>
    ## 1   134

``` r
instacart %>%
  group_by(aisle_id, aisle) %>% 
  summarise(n_obs = n()) %>% # count observations of each aisle
  arrange(desc(n_obs)) # arrange number of observations in a descending order
```

    ## # A tibble: 134 x 3
    ## # Groups:   aisle_id [134]
    ##    aisle_id aisle                          n_obs
    ##       <int> <chr>                          <int>
    ##  1       83 fresh vegetables              150609
    ##  2       24 fresh fruits                  150473
    ##  3      123 packaged vegetables fruits     78493
    ##  4      120 yogurt                         55240
    ##  5       21 packaged cheese                41699
    ##  6      115 water seltzer sparkling water  36617
    ##  7       84 milk                           32644
    ##  8      107 chips pretzels                 31269
    ##  9       91 soy lactosefree                26240
    ## 10      112 bread                          23635
    ## # ... with 124 more rows

\*\* There are 134 dinstinct aisles. The top 3 popular aisles are 83-fresh vegetables (150609 items), 24-fresh fruits (150473 items) and 123-packaged vegetables fruits (78493 items). \*\*

-   Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.

``` r
instacart %>%
  group_by(aisle_id, aisle) %>% 
  summarise(n_obs = n()) %>% # count observations of each aisle
  # mutate(aisle = forcats::fct_reorder(aisle, n_obs)) %>% 
  ggplot() + 
  geom_point(aes(x = aisle, y = n_obs)) + 
  labs(
    title = "Popularity of Aisles",
    x = "aisle",
    y = "number of items ordered",
    caption = "Data from instacart"
  )
```

![](p8105_hw3_rj2543_files/figure-markdown_github/aisle%20plot-1.png)

-   Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

``` r
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>% 
  summarise(popularity = n()) %>% 
  group_by(aisle) %>% 
  filter(popularity == max(popularity)) %>% 
  knitr::kable()
```

| aisle                      | product\_name                                 |  popularity|
|:---------------------------|:----------------------------------------------|-----------:|
| baking ingredients         | Light Brown Sugar                             |         499|
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |          30|
| packaged vegetables fruits | Organic Baby Spinach                          |        9784|

-   Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

``` r
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarise(mean_hour = mean(order_hour_of_day)) %>% 
  spread(key = order_dow, value = mean_hour) %>% # produce a 2*7 table
  knitr::kable(digits = 1) # Do we need to round the mean hour in order to be more reader-friendly?
```

| product\_name    |     0|     1|     2|     3|     4|     5|     6|
|:-----------------|-----:|-----:|-----:|-----:|-----:|-----:|-----:|
| Coffee Ice Cream |  13.8|  14.3|  15.4|  15.3|  15.2|  12.3|  13.8|
| Pink Lady Apples |  13.4|  11.4|  11.7|  14.2|  11.6|  12.8|  11.9|

Problem 3
=========

This problem uses the NY NOAA data. DO NOT include this dataset in your local data directory; instead, load the data from the p8105.datasets package (it’s called ny\_noaa).

``` r
data("ny_noaa")

ny_noaa = ny_noaa %>% 
  janitor::clean_names() %>% 
  mutate(tmax = as.numeric(tmax), tmin = as.numeric(tmin)) # convert "tmax" & "tmin" from chr to numeric

ny_noaa
```

    ## # A tibble: 2,595,176 x 7
    ##    id          date        prcp  snow  snwd  tmax  tmin
    ##    <chr>       <date>     <int> <int> <int> <dbl> <dbl>
    ##  1 US1NYAB0001 2007-11-01    NA    NA    NA    NA    NA
    ##  2 US1NYAB0001 2007-11-02    NA    NA    NA    NA    NA
    ##  3 US1NYAB0001 2007-11-03    NA    NA    NA    NA    NA
    ##  4 US1NYAB0001 2007-11-04    NA    NA    NA    NA    NA
    ##  5 US1NYAB0001 2007-11-05    NA    NA    NA    NA    NA
    ##  6 US1NYAB0001 2007-11-06    NA    NA    NA    NA    NA
    ##  7 US1NYAB0001 2007-11-07    NA    NA    NA    NA    NA
    ##  8 US1NYAB0001 2007-11-08    NA    NA    NA    NA    NA
    ##  9 US1NYAB0001 2007-11-09    NA    NA    NA    NA    NA
    ## 10 US1NYAB0001 2007-11-10    NA    NA    NA    NA    NA
    ## # ... with 2,595,166 more rows

``` r
skimr::skim(ny_noaa)
```

    ## Skim summary statistics
    ##  n obs: 2595176 
    ##  n variables: 7 
    ## 
    ## -- Variable type:character ------------------------------------------------------------------------------
    ##  variable missing complete       n min max empty n_unique
    ##        id       0  2595176 2595176  11  11     0      747
    ## 
    ## -- Variable type:Date -----------------------------------------------------------------------------------
    ##  variable missing complete       n        min        max     median
    ##      date       0  2595176 2595176 1981-01-01 2010-12-31 1997-01-21
    ##  n_unique
    ##     10957
    ## 
    ## -- Variable type:integer --------------------------------------------------------------------------------
    ##  variable missing complete       n  mean     sd  p0 p25 p50 p75  p100
    ##      prcp  145838  2449338 2595176 29.82  78.18   0   0   0  23 22860
    ##      snow  381221  2213955 2595176  4.99  27.22 -13   0   0   0 10160
    ##      snwd  591786  2003390 2595176 37.31 113.54   0   0   0   0  9195
    ##      hist
    ##  <U+2587><U+2581><U+2581><U+2581><U+2581><U+2581><U+2581><U+2581>
    ##  <U+2587><U+2581><U+2581><U+2581><U+2581><U+2581><U+2581><U+2581>
    ##  <U+2587><U+2581><U+2581><U+2581><U+2581><U+2581><U+2581><U+2581>
    ## 
    ## -- Variable type:numeric --------------------------------------------------------------------------------
    ##  variable missing complete       n   mean     sd   p0 p25 p50 p75 p100
    ##      tmax 1134358  1460818 2595176 139.8  111.42 -389  50 150 233  600
    ##      tmin 1134420  1460756 2595176  30.29 104    -594 -39  33 111  600
    ##      hist
    ##  <U+2581><U+2581><U+2582><U+2587><U+2587><U+2586><U+2581><U+2581>
    ##  <U+2581><U+2581><U+2581><U+2586><U+2587><U+2582><U+2581><U+2581>

The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and indicating the extent to which missing data is an issue. Then, do or answer the following (commenting on the results of each):

**The dataset is derived from the original data "ny\_noaa". The dataset has 2595176 rows and 7 columns. The structure of the data is tbl\_df, tbl, data.frame. Key variables include id, date, prcp, snow, snwd, tmax, tmin. Descriptions about variables can be found at** <http://p8105.com/dataset_noaa.html>:

-   id: Weather station ID
-   date: Date of observation
-   prcp: Precipitation (tenths of mm)
-   snow: Snowfall (mm)
-   snwd: Snow depth (mm)
-   tmax: Maximum temperature (tenths of degrees C)
-   tmin: Minimum temperature (tenths of degrees C)

**By using "skimr::skim(ny\_noaa)", we can see that there are 145838 (5.6195803%) missing data for prcp, 381221 (14.689601%) missing data for snow, 591786(22.8033089%) missing data for snwd , 1134358 (43.7102532%) missing data for tmax and 1134420 (43.7126422%) missing data for tmin.**

-   Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why?

``` r
ny_noaa1 = ny_noaa %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>% # create separate variables for original "date"
  mutate(year = as.integer(year), month = as.integer(month), day = as.integer(day), prcp = prcp/10, tmax = tmax/10, tmin = tmin/10) # convert "year", "month", "day" from chr to int, convert "prcp", "tmax", "tmin" from tenths to units

ny_noaa1
```

    ## # A tibble: 2,595,176 x 9
    ##    id           year month   day  prcp  snow  snwd  tmax  tmin
    ##    <chr>       <int> <int> <int> <dbl> <int> <int> <dbl> <dbl>
    ##  1 US1NYAB0001  2007    11     1    NA    NA    NA    NA    NA
    ##  2 US1NYAB0001  2007    11     2    NA    NA    NA    NA    NA
    ##  3 US1NYAB0001  2007    11     3    NA    NA    NA    NA    NA
    ##  4 US1NYAB0001  2007    11     4    NA    NA    NA    NA    NA
    ##  5 US1NYAB0001  2007    11     5    NA    NA    NA    NA    NA
    ##  6 US1NYAB0001  2007    11     6    NA    NA    NA    NA    NA
    ##  7 US1NYAB0001  2007    11     7    NA    NA    NA    NA    NA
    ##  8 US1NYAB0001  2007    11     8    NA    NA    NA    NA    NA
    ##  9 US1NYAB0001  2007    11     9    NA    NA    NA    NA    NA
    ## 10 US1NYAB0001  2007    11    10    NA    NA    NA    NA    NA
    ## # ... with 2,595,166 more rows

``` r
ny_noaa1 %>% 
  filter(!is.na(snow)) %>% # exclude NA
  group_by(snow) %>% # focus on snowfall
  summarise(n_obs = n()) %>% # count observations for each snowfall
  arrange(desc(n_obs)) # interested in most commonly observed values
```

    ## # A tibble: 281 x 2
    ##     snow   n_obs
    ##    <int>   <int>
    ##  1     0 2008508
    ##  2    25   31022
    ##  3    13   23095
    ##  4    51   18274
    ##  5    76   10173
    ##  6     8    9962
    ##  7     5    9748
    ##  8    38    9197
    ##  9     3    8790
    ## 10   102    6552
    ## # ... with 271 more rows

\*\* Despite NA and 0 mm, 25 mm, 13 mm and 51 mm are the top 3 most commonly observed values. \*\*

-   Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

``` r
ny_noaa1 %>% 
  filter(month %in% c(1,7)) %>% 
  group_by(year, month) %>% 
  summarise(mean_tmax = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = mean_tmax)) +
  geom_line() +
  facet_grid(.~month)
```

![](p8105_hw3_rj2543_files/figure-markdown_github/average%20tmax%20in%20Jan%20&%20Jul-1.png)

-   Make a two-panel plot showing

1.  tmax vs tmin for the full dataset (note that a scatterplot may not be the best option);

2.  make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

``` r
tmax_tmin = ny_noaa1 %>% 
  filter(!is.na(tmax) & !is.na(tmin)) %>% 
  ggplot(aes(x = tmax, y = tmin, color = year)) +
  geom_smooth(se = FALSE)

snowfall_distribution = ny_noaa1 %>% 
  mutate(year = as.factor(year)) %>% 
  filter(snow > 0 & snow < 100) %>% 
  ggplot(aes(x = year, y = snow)) +
  geom_violin(aes(fill = year)) +
  theme(legend.position = "bottom")

tmax_tmin/snowfall_distribution
```

    ## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'

![](p8105_hw3_rj2543_files/figure-markdown_github/2-panel%20plot-1.png)
